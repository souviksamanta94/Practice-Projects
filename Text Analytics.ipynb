{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\saman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorizer Parameters\n",
      " CountVectorizer(analyzer=<function my_analyzer at 0x000002A0AFC87378>,\n",
      "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
      "        encoding='utf-8', input='content', lowercase=True, max_df=0.95,\n",
      "        max_features=None, min_df=2, ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None) \n",
      "\n",
      "\n",
      "TF-IDF Parameters\n",
      " {'norm': 'l2', 'smooth_idf': True, 'sublinear_tf': False, 'use_idf': True} \n",
      "\n",
      "\n",
      "TF_IDF Vectorizer Parameters\n",
      " TfidfVectorizer(analyzer=<function my_analyzer at 0x000002A0AFC87378>,\n",
      "        binary=False, decode_error='strict', dtype=<class 'numpy.float64'>,\n",
      "        encoding='utf-8', input='content', lowercase=True, max_df=0.95,\n",
      "        max_features=None, min_df=2, ngram_range=(1, 2), norm='l2',\n",
      "        preprocessor=None, smooth_idf=True, stop_words=None,\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) \n",
      "\n",
      "Number of Reviews..... 13135\n",
      "Number of Terms.......  6263\n",
      "\n",
      "Topics Identified using LDA with TF_IDF\n",
      "Topic #0: wine flavor tannin black blackberry cabernet currant oak year fruit cherry dry rich show ripe\n",
      "\n",
      "Topic #1: barely wait sweaty bay overpower weave chile front tongue create funky drop generosity acceptable underbelly\n",
      "\n",
      "Topic #2: meet coconut tightly party wound lend fade saddle beneath easygoing pleasantly well-balanced subdue bread small-production\n",
      "\n",
      "Topic #3: punch expansive cardamom coast aromatics boast handful enjoyment central tomato amidst waft thickness cracker graham\n",
      "\n",
      "Topic #4: flavor blackberry cherry dry soft drink wine sweet oak cabernet finish tannin good cab ripe\n",
      "\n",
      "Topic #5: palate petit verdot nose merlot malbec small franc amount blend leather juicy tar pepper tobacco\n",
      "\n",
      "Topic #6: sirah petite cherry-berry bottling showy reduce figure appropriately curiously provenance dark-fruit root awash lightness pipe\n",
      "\n",
      "Topic #7: brightness weedy muscular breadth recall farm opposite cake black-fruit relieve neighbor lohr j. six-plus today\n",
      "\n",
      "Topic #8: bouquet effort santa light-bodied elevation lurk loam slate ting notion excite gamy offset medium-weight reduction\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:185: FutureWarning: Using 'rename_axis' to alter labels is deprecated. Use '.rename' instead\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Region</th>\n",
       "      <th>California Other</th>\n",
       "      <th>Central Coast</th>\n",
       "      <th>Central Valley</th>\n",
       "      <th>Clear Lake</th>\n",
       "      <th>High Valley</th>\n",
       "      <th>Lake County</th>\n",
       "      <th>Mendocino</th>\n",
       "      <th>Mendocino County</th>\n",
       "      <th>Mendocino Ridge</th>\n",
       "      <th>Mendocino/Lake Counties</th>\n",
       "      <th>Napa</th>\n",
       "      <th>Napa-Sonoma</th>\n",
       "      <th>North Coast</th>\n",
       "      <th>Red Hills Lake County</th>\n",
       "      <th>Redwood Valley</th>\n",
       "      <th>Sierra Foothills</th>\n",
       "      <th>Sonoma</th>\n",
       "      <th>South Coast</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic#</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.77</td>\n",
       "      <td>50.70</td>\n",
       "      <td>33.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>62.07</td>\n",
       "      <td>66.67</td>\n",
       "      <td>56.12</td>\n",
       "      <td>78.31</td>\n",
       "      <td>70.24</td>\n",
       "      <td>36.07</td>\n",
       "      <td>64.86</td>\n",
       "      <td>66.67</td>\n",
       "      <td>48.41</td>\n",
       "      <td>65.22</td>\n",
       "      <td>42.31</td>\n",
       "      <td>67.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.22</td>\n",
       "      <td>43.62</td>\n",
       "      <td>61.58</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>36.67</td>\n",
       "      <td>34.48</td>\n",
       "      <td>33.33</td>\n",
       "      <td>42.86</td>\n",
       "      <td>18.43</td>\n",
       "      <td>21.43</td>\n",
       "      <td>58.47</td>\n",
       "      <td>35.14</td>\n",
       "      <td>33.33</td>\n",
       "      <td>45.24</td>\n",
       "      <td>30.79</td>\n",
       "      <td>44.23</td>\n",
       "      <td>29.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.34</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2.98</td>\n",
       "      <td>8.33</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.56</td>\n",
       "      <td>3.29</td>\n",
       "      <td>13.46</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Region  California Other  Central Coast  Central Valley  Clear Lake  \\\n",
       "Topic#                                                                \n",
       "0                  26.77          50.70           33.99         0.0   \n",
       "1                   0.00           0.17            0.99         0.0   \n",
       "2                   0.00           0.28            0.99         0.0   \n",
       "3                   0.00           0.00            0.00         0.0   \n",
       "4                  71.22          43.62           61.58       100.0   \n",
       "5                   1.34           5.00            2.46         0.0   \n",
       "6                   0.27           0.00            0.00         0.0   \n",
       "7                   0.00           0.00            0.00         0.0   \n",
       "8                   0.40           0.22            0.00         0.0   \n",
       "All               100.00         100.00          100.00       100.0   \n",
       "\n",
       "Region  High Valley  Lake County  Mendocino  Mendocino County  \\\n",
       "Topic#                                                          \n",
       "0               0.0         50.0      60.00             62.07   \n",
       "1               0.0          0.0       0.00              0.00   \n",
       "2               0.0          0.0       3.33              0.00   \n",
       "3               0.0          0.0       0.00              0.00   \n",
       "4             100.0         50.0      36.67             34.48   \n",
       "5               0.0          0.0       0.00              3.45   \n",
       "6               0.0          0.0       0.00              0.00   \n",
       "7               0.0          0.0       0.00              0.00   \n",
       "8               0.0          0.0       0.00              0.00   \n",
       "All           100.0        100.0     100.00            100.00   \n",
       "\n",
       "Region  Mendocino Ridge  Mendocino/Lake Counties    Napa  Napa-Sonoma  \\\n",
       "Topic#                                                                  \n",
       "0                 66.67                    56.12   78.31        70.24   \n",
       "1                  0.00                     0.00    0.00         0.00   \n",
       "2                  0.00                     0.51    0.14         0.00   \n",
       "3                  0.00                     0.00    0.03         0.00   \n",
       "4                 33.33                    42.86   18.43        21.43   \n",
       "5                  0.00                     0.51    2.98         8.33   \n",
       "6                  0.00                     0.00    0.04         0.00   \n",
       "7                  0.00                     0.00    0.05         0.00   \n",
       "8                  0.00                     0.00    0.03         0.00   \n",
       "All              100.00                   100.00  100.00       100.00   \n",
       "\n",
       "Region  North Coast  Red Hills Lake County  Redwood Valley  Sierra Foothills  \\\n",
       "Topic#                                                                         \n",
       "0             36.07                  64.86           66.67             48.41   \n",
       "1              1.09                   0.00            0.00              0.00   \n",
       "2              0.00                   0.00            0.00              0.00   \n",
       "3              0.00                   0.00            0.00              0.00   \n",
       "4             58.47                  35.14           33.33             45.24   \n",
       "5              4.37                   0.00            0.00              5.56   \n",
       "6              0.00                   0.00            0.00              0.79   \n",
       "7              0.00                   0.00            0.00              0.00   \n",
       "8              0.00                   0.00            0.00              0.00   \n",
       "All          100.00                 100.00          100.00            100.00   \n",
       "\n",
       "Region  Sonoma  South Coast     All  \n",
       "Topic#                               \n",
       "0        65.22        42.31   67.07  \n",
       "1         0.31         0.00    0.11  \n",
       "2         0.00         0.00    0.14  \n",
       "3         0.00         0.00    0.02  \n",
       "4        30.79        44.23   29.19  \n",
       "5         3.29        13.46    3.27  \n",
       "6         0.22         0.00    0.08  \n",
       "7         0.18         0.00    0.06  \n",
       "8         0.00         0.00    0.07  \n",
       "All     100.00       100.00  100.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr  2 21:55:50 2019\n",
    "\n",
    "@author: souviksamanta\n",
    "\"\"\"\n",
    "#Importing Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# my_analyzer replaces both the preprocessor and tokenizer and replaces stop word removal and ngram constructions\n",
    "\n",
    "def my_analyzer(s):\n",
    "    # Synonym List\n",
    "    syns = {'veh': 'vehicle', 'car': 'vehicle', 'chev':'cheverolet', \\\n",
    "              'chevy':'cheverolet', 'air bag': 'airbag', \\\n",
    "              'seat belt':'seatbelt', \"n't\":'not', 'to30':'to 30', \\\n",
    "              'wont':'would not', 'cant':'can not', 'cannot':'can not', \\\n",
    "              'couldnt':'could not', 'shouldnt':'should not', \\\n",
    "              'wouldnt':'would not', }\n",
    "    \n",
    "    # Preprocess String s\n",
    "    s = s.lower()\n",
    "    s = s.replace(',', '. ')\n",
    "    # Tokenize \n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = [word.replace(',','') for word in tokens ]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and \\\n",
    "              (\"''\" != word) and (\"``\" != word) and \\\n",
    "              (word!='description') and (word !='dtype') \\\n",
    "              and (word != 'object') and (word!=\"'s\")]\n",
    "    \n",
    "    # Map synonyms\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in syns:\n",
    "            tokens[i] = syns[tokens[i]]\n",
    "            \n",
    "    # Remove stop words\n",
    "    punctuation = list(string.punctuation)+['..', '...']\n",
    "    pronouns = ['i', 'he', 'she', 'it', 'him', 'they', 'we', 'us', 'them']\n",
    "    stop = stopwords.words('english') + punctuation + pronouns\n",
    "    filtered_terms = [word for word in tokens if (word not in stop) and \\\n",
    "                  (len(word)>1) and (not word.replace('.','',1).isnumeric()) \\\n",
    "                  and (not word.replace(\"'\",'',2).isnumeric())]\n",
    "    \n",
    "    # Lemmatization & Stemming - Stemming with WordNet POS\n",
    "    # Since lemmatization requires POS need to set POS\n",
    "    tagged_words = pos_tag(filtered_terms, lang='eng')\n",
    "    # Stemming with for terms without WordNet POS\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "    wnl = WordNetLemmatizer()\n",
    "    stemmed_tokens = []\n",
    "    for tagged_token in tagged_words:\n",
    "        term = tagged_token[0]\n",
    "        pos  = tagged_token[1]\n",
    "        pos  = pos[0]\n",
    "        try:\n",
    "            pos   = wn_tags[pos]\n",
    "            stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "        except:\n",
    "            stemmed_tokens.append(stemmer.stem(term))\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Further Customization of Stopping and Stemming using NLTK\n",
    "def my_preprocessor(s):\n",
    "    #Vectorizer sends one string at a time\n",
    "    s = s.lower()\n",
    "    s = s.replace(',', '. ')\n",
    "    print(\"preprocessor\")\n",
    "    return(s)\n",
    "    \n",
    "def my_tokenizer(s):\n",
    "    # Tokenize\n",
    "    print(\"Tokenizer\")\n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = [word.replace(',','') for word in tokens ]\n",
    "    tokens = [word for word in tokens if word.find('*')!=True and \\\n",
    "              word != \"''\" and word !=\"``\" and word!='description' \\\n",
    "              and word !='dtype']\n",
    "    return tokens\n",
    "\n",
    "# Increase Pandas column width to let pandas read large text columns\n",
    "pd.set_option('max_colwidth', 32575)\n",
    "# Read GMC Ignition Recall Comments from NTHSA Data\n",
    "\n",
    "df = pd.read_excel(\"C:/Users/saman/OneDrive/Texas A&M/656/Python/CaliforniaCabernet.xlsx\")\n",
    "\n",
    "# Setup simple constants\n",
    "n_docs     = len(df['description'])\n",
    "n_samples  = n_docs\n",
    "m_features = None\n",
    "s_words    = 'english'\n",
    "ngram = (1,2)\n",
    "\n",
    "# Setup reviews in list 'discussions'\n",
    "discussions = []\n",
    "for i in range(n_samples):\n",
    "    discussions.append((\"%s\" %df['description'].iloc[i]))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Create Word Frequency by Review Matrix using Custom Analyzer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, max_features=m_features,\\\n",
    "                     analyzer=my_analyzer, ngram_range=ngram)\n",
    "tf = cv.fit_transform(discussions)\n",
    "\n",
    "print(\"\\nVectorizer Parameters\\n\", cv, \"\\n\")\n",
    "\n",
    "\n",
    "n_topics        = 9\n",
    "max_iter        =  5\n",
    "learning_offset = 20.\n",
    "learning_method = 'online'\n",
    "\n",
    "# LDA for TF-IDF x Doc Matrix\n",
    "# First Create Term-Frequency/Inverse Doc Frequency by Review Matrix\n",
    "# This requires constructing Term Freq. x Doc. matrix first\n",
    "\n",
    "tf_idf = TfidfTransformer()\n",
    "print(\"\\nTF-IDF Parameters\\n\", tf_idf.get_params(),\"\\n\")\n",
    "tf_idf = tf_idf.fit_transform(tf)\n",
    "\n",
    "# Or you can construct the TF/IDF matrix from the data\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.95, min_df=2, max_features=m_features,\\\n",
    "                             analyzer=my_analyzer, ngram_range=ngram)\n",
    "tf_idf = tfidf_vect.fit_transform(discussions)\n",
    "print(\"\\nTF_IDF Vectorizer Parameters\\n\", tfidf_vect, \"\\n\")\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter,\\\n",
    "                                learning_method=learning_method, \\\n",
    "                                learning_offset=learning_offset, \\\n",
    "                                random_state=12345)\n",
    "lda.fit_transform(tf_idf)\n",
    "print('{:.<22s}{:>6d}'.format(\"Number of Reviews\", tf.shape[0]))\n",
    "print('{:.<22s}{:>6d}'.format(\"Number of Terms\",     tf.shape[1]))\n",
    "print(\"\\nTopics Identified using LDA with TF_IDF\")\n",
    "tf_features = cv.get_feature_names()\n",
    "max_words = 15\n",
    "topic_description=[]\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([tf_features[i]\n",
    "                             for i in topic.argsort()[:-max_words - 1:-1]])\n",
    "        topic_description.append(message[10:])\n",
    "        print(message)\n",
    "        print()\n",
    "        \n",
    "for i in range(len(topic_description)):\n",
    "    topic_description[i]=topic_description[i].split(' ')\n",
    "    \n",
    "temp=lda.transform(tf_idf)\n",
    "temp1=[]\n",
    "for i in range(len(temp)):\n",
    "    temp1.append(temp[i].argmax())\n",
    "temp1=pd.DataFrame(temp1,columns=['Topic#'])\n",
    "df=df.join(temp1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "table1=df.pivot_table(['points','price'],index='Topic#')\n",
    "table1=table1.join(pd.DataFrame(topic_description))\n",
    "\n",
    "#Table 1 with average points, average price and 15 words of cluster description\n",
    "table1=table1.rename_axis({'points':'avg_points','price':'avg_price'},axis=1)\n",
    "table1.T\n",
    "\n",
    "table2=df.pivot_table('Review',index='Region',columns='Topic#',\\\n",
    "                      aggfunc='count',\\\n",
    "                      fill_value=0,margins=True)\n",
    "#Table 2 with Percent of reviews in that region associated with each of 9 clusters\n",
    "def percent_convert(x):\n",
    "    for index in x.index:\n",
    "        for i in x.columns:\n",
    "            x.loc[index,i]=round(x.loc[index,i]*100/x.loc[index,'All'],2)\n",
    "    \n",
    "    return x\n",
    "percent_convert(table2)\n",
    "table2.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
